# 03 - Web Scraping ile Veri Toplama

## ğŸ¯ Bu BÃ¶lÃ¼mde Neler Ã–ÄŸreneceÄŸiz?

1. Crawl4AI nedir ve neden kullanÄ±yoruz?
2. Basit web scraping iÅŸlemleri
3. Asenkron (paralel) scraping
4. LLM-friendly veri Ã§Ä±karma
5. Crawl4AI'yÄ± ADK ajanlarÄ±na entegre etme

---

## Neden Crawl4AI?

### Geleneksel YÃ¶ntemler vs Crawl4AI

| Ã–zellik | requests + BeautifulSoup | Selenium | Crawl4AI |
|---------|-------------------------|----------|----------|
| JavaScript desteÄŸi | âŒ Yok | âœ… Var | âœ… Var |
| HÄ±z | âš¡ Ã‡ok hÄ±zlÄ± | ğŸŒ YavaÅŸ | âš¡ HÄ±zlÄ± |
| Async/Paralel | ğŸ”§ Manuel | ğŸ”§ Zor | âœ… HazÄ±r |
| LLM-friendly Ã§Ä±ktÄ± | ğŸ”§ Manuel temizlik | ğŸ”§ Manuel temizlik | âœ… Otomatik Markdown |
| AI ajanlarÄ± iÃ§in tasarÄ±m | âŒ | âŒ | âœ… |

### Crawl4AI'Ä±n AvantajlarÄ±

1. **Markdown Ã‡Ä±ktÄ±sÄ±**: HTML yerine temiz Markdown â†’ LLM'e daha az token
2. **AkÄ±llÄ± Temizleme**: MenÃ¼, reklam, footer otomatik temizlenir
3. **JavaScript DesteÄŸi**: Modern SPA'ler (React, Vue) Ã§alÄ±ÅŸÄ±r
4. **Asenkron**: AynÄ± anda 10-20 site taranabilir
5. **AÃ§Ä±k Kaynak**: Ãœcretsiz, Ã¶zelleÅŸtirilebilir

---

## AdÄ±m 1: Crawl4AI Kurulumu (Tekrar)

EÄŸer kurulum bÃ¶lÃ¼mÃ¼nde yaptÄ±ysanÄ±z atla, yoksa:

```bash
# Sanal ortamÄ± aktifleÅŸtir
source venv/bin/activate

# Crawl4AI'yÄ± kur
pip install 'crawl4ai[all]'

# Playwright browser'Ä± kur
playwright install chromium
```

### Test Edelim

```bash
python -c "from crawl4ai import AsyncWebCrawler; print('âœ… Crawl4AI hazÄ±r!')"
```

---

## AdÄ±m 2: Ä°lk Scraping Ä°ÅŸlemi

### Dosya: `examples/04_basic_scraping.py`

```python
"""
Crawl4AI ile basit web scraping
"""

import asyncio
from crawl4ai import AsyncWebCrawler

async def scrape_single_page(url: str):
    """Tek bir sayfayÄ± tarar"""
    
    print(f"ğŸŒ Taranan URL: {url}\n")
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        # SayfayÄ± tara
        result = await crawler.arun(url=url)
        
        # SonuÃ§larÄ± gÃ¶ster
        print("=" * 70)
        print("ğŸ“Š SCRAPING SONUÃ‡LARI")
        print("=" * 70)
        print(f"âœ… BaÅŸarÄ±lÄ±: {result.success}")
        print(f"ğŸ“ HTML boyutu: {len(result.html)} karakter")
        print(f"ğŸ“ Markdown boyutu: {len(result.markdown)} karakter")
        print(f"ğŸ”— Bulunan link sayÄ±sÄ±: {len(result.links)}")
        
        print("\n" + "=" * 70)
        print("ğŸ“„ Ä°Ã‡ERÄ°K Ã–NÄ°ZLEMESÄ° (Ä°lk 1000 karakter)")
        print("=" * 70)
        print(result.markdown[:1000])
        print("\n...")
        
        return result

def main():
    print("=" * 70)
    print("CRAWL4AI - TEMEL WEB SCRAPING")
    print("=" * 70 + "\n")
    
    # Test URL'i (Wikipedia - stabil ve hÄ±zlÄ±)
    test_url = "https://en.wikipedia.org/wiki/Artificial_intelligence"
    
    # Async fonksiyonu Ã§alÄ±ÅŸtÄ±r
    result = asyncio.run(scrape_single_page(test_url))
    
    print("\nâœ… Scraping tamamlandÄ±!")

if __name__ == "__main__":
    main()
```

### Ã‡alÄ±ÅŸtÄ±rma

```bash
python examples/04_basic_scraping.py
```

### Ne Oldu?

1. Crawl4AI bir Chromium browser aÃ§tÄ± (gÃ¶rÃ¼nmez - headless)
2. Wikipedia sayfasÄ±nÄ± yÃ¼kledi
3. JavaScript'leri Ã§alÄ±ÅŸtÄ±rdÄ±
4. Ä°Ã§eriÄŸi temizleyip Markdown'a Ã§evirdi
5. Link'leri, gÃ¶rselleri extract etti

---

## AdÄ±m 3: Paralel (Asenkron) Scraping

GerÃ§ek "Deep Research" iÃ§in tek tek taramak Ã§ok yavaÅŸ. Crawl4AI birden fazla siteyi aynÄ± anda tarayabilir.

### Dosya: `examples/05_parallel_scraping.py`

```python
"""
Birden fazla URL'i paralel olarak tara
"""

import asyncio
from crawl4ai import AsyncWebCrawler
from typing import List, Dict

async def scrape_multiple_urls(urls: List[str]) -> Dict[str, str]:
    """
    Birden fazla URL'i paralel olarak tarar
    
    Args:
        urls: Taranacak URL listesi
        
    Returns:
        {url: markdown_content} dictionary
    """
    
    results = {}
    
    async with AsyncWebCrawler(verbose=False) as crawler:
        print(f"ğŸš€ {len(urls)} URL paralel olarak taranÄ±yor...\n")
        
        # TÃ¼m URL'ler iÃ§in task oluÅŸtur
        tasks = [crawler.arun(url=url) for url in urls]
        
        # Hepsini aynÄ± anda Ã§alÄ±ÅŸtÄ±r ve bekle
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        # SonuÃ§larÄ± iÅŸle
        for url, response in zip(urls, responses):
            if isinstance(response, Exception):
                print(f"âŒ Hata [{url}]: {response}")
                results[url] = f"ERROR: {response}"
            elif response.success:
                print(f"âœ… BaÅŸarÄ±lÄ± [{url}] - {len(response.markdown)} karakter")
                results[url] = response.markdown
            else:
                print(f"âš ï¸  BaÅŸarÄ±sÄ±z [{url}]")
                results[url] = "ERROR: Scraping failed"
    
    return results

def main():
    print("=" * 70)
    print("PARALEL WEB SCRAPING")
    print("=" * 70 + "\n")
    
    # Test URL'leri
    urls = [
        "https://en.wikipedia.org/wiki/Electric_vehicle",
        "https://en.wikipedia.org/wiki/Lithium-ion_battery",
        "https://en.wikipedia.org/wiki/Tesla,_Inc.",
    ]
    
    print(f"ğŸ“‹ Taranacak URL sayÄ±sÄ±: {len(urls)}\n")
    
    # Paralel scraping
    import time
    start_time = time.time()
    
    results = asyncio.run(scrape_multiple_urls(urls))
    
    elapsed = time.time() - start_time
    
    print(f"\nâ±ï¸  Toplam sÃ¼re: {elapsed:.2f} saniye")
    print(f"ğŸ“Š Ortalama: {elapsed/len(urls):.2f} saniye/URL")
    
    print("\n" + "=" * 70)
    print("SONUÃ‡LAR")
    print("=" * 70)
    
    for url, content in results.items():
        preview = content[:200] if not content.startswith("ERROR") else content
        print(f"\nğŸ”— {url}")
        print(f"   ğŸ“ {len(content)} karakter")
        print(f"   ğŸ‘ï¸  Ã–nizleme: {preview}...")

if __name__ == "__main__":
    main()
```

### Ã‡alÄ±ÅŸtÄ±rma

```bash
python examples/05_parallel_scraping.py
```

### Seri vs Paralel KarÅŸÄ±laÅŸtÄ±rma

| YÃ¶ntem | 3 URL iÃ§in SÃ¼re | 10 URL iÃ§in SÃ¼re |
|--------|-----------------|------------------|
| Seri (tek tek) | ~15 saniye | ~50 saniye |
| Paralel (aynÄ± anda) | ~5 saniye | ~8 saniye |

---

## AdÄ±m 4: LLM-Friendly Scraping

Crawl4AI'Ä±n en gÃ¼Ã§lÃ¼ Ã¶zelliÄŸi: Sadece bize deÄŸil, **LLM'e** de uygun iÃ§erik Ã§Ä±karmasÄ±.

### Ã–zellikler

1. **Otomatik Temizleme**: Gereksiz elementler Ã§Ä±karÄ±lÄ±r
2. **YapÄ±landÄ±rÄ±lmÄ±ÅŸ Markdown**: BaÅŸlÄ±klar, listeler, tablolar korunur
3. **Link Extraction**: TÃ¼m link'ler ayrÄ±ca listelenir
4. **Media Extraction**: GÃ¶rseller, videolar metadata olarak

### GeliÅŸmiÅŸ Scraping AyarlarÄ±

### Dosya: `src/tools/advanced_scraper.py`

```python
"""
GeliÅŸmiÅŸ scraping fonksiyonlarÄ±
LLM'ler iÃ§in optimize edilmiÅŸ
"""

import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from typing import List, Dict, Optional
import os
from dotenv import load_dotenv

load_dotenv()

class AdvancedScraper:
    """GeliÅŸmiÅŸ web scraping sÄ±nÄ±fÄ±"""
    
    def __init__(self, verbose: bool = False):
        self.verbose = verbose
        
        # Browser konfigÃ¼rasyonu
        self.browser_config = BrowserConfig(
            headless=True,  # GÃ¶rÃ¼nmez mod
            viewport_width=1920,
            viewport_height=1080,
            user_agent="Mozilla/5.0 (Martur Research Agent)"
        )
    
    async def scrape_for_llm(
        self, 
        url: str,
        wait_for: Optional[str] = None  # CSS selector - beklenecek element
    ) -> Dict[str, any]:
        """
        LLM iÃ§in optimize edilmiÅŸ scraping
        
        Args:
            url: Taranacak URL
            wait_for: JavaScript yÃ¼klenmesini beklemek iÃ§in CSS selector
            
        Returns:
            TemizlenmiÅŸ iÃ§erik ve metadata
        """
        
        async with AsyncWebCrawler(
            config=self.browser_config,
            verbose=self.verbose
        ) as crawler:
            
            # Crawler ayarlarÄ±
            run_config = CrawlerRunConfig(
                # JavaScript'lerin yÃ¼klenmesini bekle
                js_code="window.scrollTo(0, document.body.scrollHeight);",  # SayfayÄ± kaydÄ±r
                wait_for=wait_for,
                
                # Sadece ana iÃ§eriÄŸi al
                exclude_external_links=True,  # DÄ±ÅŸ link'leri Ã§Ä±kar
                
                # Medya ayarlarÄ±
                screenshot=False,  # Screenshot'a gerek yok (token tasarrufu)
            )
            
            result = await crawler.arun(
                url=url,
                config=run_config
            )
            
            if not result.success:
                return {
                    "success": False,
                    "error": "Scraping failed",
                    "url": url
                }
            
            # LLM-friendly output
            return {
                "success": True,
                "url": url,
                "title": self._extract_title(result.html),
                "markdown": result.markdown,
                "markdown_length": len(result.markdown),
                "links": result.links[:10],  # Ä°lk 10 link
                "summary": result.markdown[:500]  # Ä°lk 500 karakter Ã¶zet
            }
    
    async def scrape_multiple_for_llm(
        self,
        urls: List[str],
        max_concurrent: int = 5
    ) -> List[Dict[str, any]]:
        """
        Birden fazla URL'i paralel tara (rate limit ile)
        
        Args:
            urls: URL listesi
            max_concurrent: AynÄ± anda maksimum tarama sayÄ±sÄ±
            
        Returns:
            SonuÃ§ listesi
        """
        
        results = []
        
        # URL'leri gruplara bÃ¶l (rate limiting iÃ§in)
        for i in range(0, len(urls), max_concurrent):
            batch = urls[i:i + max_concurrent]
            
            print(f"ğŸ”„ Batch {i//max_concurrent + 1} taranÄ±yor ({len(batch)} URL)...")
            
            tasks = [self.scrape_for_llm(url) for url in batch]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append({"success": False, "error": str(result)})
                else:
                    results.append(result)
            
            # Batch'ler arasÄ± bekleme (rate limit)
            if i + max_concurrent < len(urls):
                await asyncio.sleep(1)
        
        return results
    
    def _extract_title(self, html: str) -> str:
        """HTML'den title tag'ini Ã§Ä±kar"""
        import re
        match = re.search(r'<title>(.*?)</title>', html, re.IGNORECASE)
        return match.group(1) if match else "No title"


# Test fonksiyonu
async def test_advanced_scraper():
    """Test iÃ§in Ã¶rnek kullanÄ±m"""
    
    scraper = AdvancedScraper(verbose=True)
    
    # Tek URL test
    result = await scraper.scrape_for_llm(
        "https://en.wikipedia.org/wiki/Web_scraping"
    )
    
    print("\n" + "=" * 70)
    print("TEST SONUCU")
    print("=" * 70)
    print(f"BaÅŸarÄ±lÄ±: {result['success']}")
    print(f"BaÅŸlÄ±k: {result['title']}")
    print(f"Ä°Ã§erik uzunluÄŸu: {result['markdown_length']} karakter")
    print(f"\nÃ–zet:\n{result['summary']}")

if __name__ == "__main__":
    asyncio.run(test_advanced_scraper())
```

### Test

```bash
python src/tools/advanced_scraper.py
```

---

## AdÄ±m 5: Scraper'Ä± ADK AjanÄ±na Entegre Etme

Åimdi bu scraper'Ä± bir ADK ajanÄ±nÄ±n kullanabileceÄŸi tool'a dÃ¶nÃ¼ÅŸtÃ¼relim.

### Dosya: `src/tools/scraping_tools.py`

```python
"""
ADK ajanlarÄ± iÃ§in scraping tool'larÄ±
"""

import asyncio
from typing import List, Dict
from .advanced_scraper import AdvancedScraper

# Global scraper instance (performans iÃ§in)
_scraper = AdvancedScraper(verbose=False)

def scrape_url_sync(url: str) -> Dict[str, any]:
    """
    Tek bir URL'i tara (senkron - ADK iÃ§in)
    
    Bu fonksiyon ADK FunctionTool olarak kullanÄ±lacak.
    ADK async desteklese de, basitlik iÃ§in sync versiyon.
    
    Args:
        url: Taranacak web sayfasÄ± URL'i
        
    Returns:
        Ä°Ã§erik ve metadata
    """
    # Async fonksiyonu sync Ã§aÄŸÄ±r
    result = asyncio.run(_scraper.scrape_for_llm(url))
    
    # LLM'e gÃ¶nderilecek formatta dÃ¶ndÃ¼r
    if result["success"]:
        return {
            "status": "success",
            "url": url,
            "title": result["title"],
            "content": result["markdown"][:10000],  # Ä°lk 10k karakter (token limiti)
            "content_length": result["markdown_length"]
        }
    else:
        return {
            "status": "error",
            "url": url,
            "error": result.get("error", "Unknown error")
        }

def scrape_multiple_urls_sync(urls: List[str]) -> List[Dict[str, any]]:
    """
    Birden fazla URL'i paralel tara (senkron wrapper)
    
    Args:
        urls: URL listesi (maksimum 10)
        
    Returns:
        Her URL iÃ§in sonuÃ§ listesi
    """
    # GÃ¼venlik: Maksimum 10 URL
    urls = urls[:10]
    
    # Async fonksiyonu Ã§alÄ±ÅŸtÄ±r
    results = asyncio.run(_scraper.scrape_multiple_for_llm(urls))
    
    # Format dÃ¶nÃ¼ÅŸÃ¼mÃ¼
    formatted_results = []
    for result in results:
        if result["success"]:
            formatted_results.append({
                "status": "success",
                "url": result["url"],
                "title": result["title"],
                "content": result["markdown"][:5000],  # 5k karakter (birden fazla URL olduÄŸu iÃ§in daha kÄ±sa)
                "content_length": result["markdown_length"]
            })
        else:
            formatted_results.append({
                "status": "error",
                "url": result.get("url", "unknown"),
                "error": result.get("error", "Unknown error")
            })
    
    return formatted_results
```

### Ajanla KullanÄ±mÄ±

### Dosya: `examples/06_agent_with_scraper.py`

```python
"""
Scraping tool'u kullanan ajan
"""

import os
import sys
from dotenv import load_dotenv

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from google.adk.agents import LlmAgent
from google.adk.models import GeminiModel
from google.adk.tools import FunctionTool

from src.tools.scraping_tools import scrape_url_sync

load_dotenv()

def create_scraping_agent():
    """Web scraping yapabilen bir ajan"""
    
    model = GeminiModel(
        model_id="gemini-1.5-flash",
        api_key=os.getenv("GOOGLE_API_KEY")
    )
    
    # Scraping tool'unu ekle
    scrape_tool = FunctionTool(scrape_url_sync)
    
    agent = LlmAgent(
        model=model,
        name="WebScrapingAgent",
        instruction="""
        Sen bir web iÃ§erik analiz uzmanÄ±sÄ±n.
        
        KullanÄ±cÄ± bir URL verdiÄŸinde:
        1. scrape_url_sync tool'unu kullanarak sayfayÄ± oku
        2. Ä°Ã§eriÄŸi analiz et
        3. Ana noktalarÄ± Ã¶zetleyerek sun
        
        KullanÄ±cÄ±nÄ±n sorusuna gÃ¶re iÃ§erikten ilgili bilgileri Ã§Ä±kar.
        """,
        tools=[scrape_tool]
    )
    
    return agent

def main():
    print("=" * 70)
    print("WEB SCRAPING AJANI")
    print("=" * 70 + "\n")
    
    agent = create_scraping_agent()
    
    # Test
    url = "https://en.wikipedia.org/wiki/Machine_learning"
    query = f"Bu sayfayÄ± oku ve machine learning'in ne olduÄŸunu Ã¶zetle: {url}"
    
    print(f"ğŸ‘¤ KullanÄ±cÄ±: {query}\n")
    print("ğŸ¤– Ajan Ã§alÄ±ÅŸÄ±yor...\n")
    
    result = agent.run(query)
    
    print("=" * 70)
    print("YANIT")
    print("=" * 70)
    print(result.output)

if __name__ == "__main__":
    main()
```

### Test

```bash
python examples/06_agent_with_scraper.py
```

---

## ğŸ“Š Performans Ä°puÃ§larÄ±

### 1. Token YÃ¶netimi

```python
# âŒ KÃ¶tÃ¼: TÃ¼m HTML'i LLM'e gÃ¶nderme
content = result.html  # 50,000+ karakter

# âœ… Ä°yi: Sadece Markdown ve kÄ±salt
content = result.markdown[:10000]  # 10k karakter
```

### 2. Rate Limiting

```python
# AynÄ± domain'e Ã§ok istek gÃ¶nderme
await asyncio.sleep(1)  # Ä°stekler arasÄ± bekleme
```

### 3. Error Handling

```python
try:
    result = await crawler.arun(url)
except Exception as e:
    print(f"Scraping hatasÄ±: {e}")
    return default_response
```

---

## ğŸ“ Ã–zet

Bu bÃ¶lÃ¼mde Ã¶ÄŸrendikleriniz:

- âœ… Crawl4AI ile temel scraping
- âœ… Paralel/asenkron scraping
- âœ… LLM-friendly iÃ§erik Ã§Ä±karma
- âœ… ADK tool olarak entegrasyon

---

## ğŸ§ª AlÄ±ÅŸtÄ±rma

AÅŸaÄŸÄ±daki gÃ¶revi tamamlayÄ±n:

1. `examples/06_agent_with_scraper.py`'yi Ã§alÄ±ÅŸtÄ±rÄ±n
2. FarklÄ± bir URL ile test edin
3. Ajana "Bu sayfadaki tablolarÄ± listele" gibi Ã¶zel bir gÃ¶rev verin

---

## ğŸ‰ Tebrikler!

Web scraping altyapÄ±nÄ±z hazÄ±r! ArtÄ±k gerÃ§ek "Deep Research" ajanÄ±nÄ± inÅŸa etmeye hazÄ±rsÄ±nÄ±z.

**SÄ±radaki AdÄ±m**: [04-research-agent.md](./04-research-agent.md) - Tam teÅŸekkÃ¼llÃ¼ araÅŸtÄ±rma ajanÄ±
