"""
Researcher Agent - Web AraÅŸtÄ±rmacÄ±sÄ±
====================================

GÃ¶rev: Verilen konuyu web'de araÅŸtÄ±r, kaynak topla, analiz et

Yetenekler:
- Web search (Tavily)
- Web scraping (Crawl4AI)
- Ä°Ã§erik analizi (Gemini)
- Multi-source sentez
"""

import os
import sys
import asyncio
import time
from typing import List, Dict
from dotenv import load_dotenv
import google.generativeai as genai
from crawl4ai import AsyncWebCrawler

# Proje root'unu path'e ekle
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from src.tools.web_tools import search_web_simple
from src.utils.source_scorer import SourceScorer
from src.utils.logger import logger, log_agent_action

load_dotenv()
genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))


class ResearcherAgent:
    """Web araÅŸtÄ±rmasÄ± yapan ajan"""
    
    def __init__(self, model_name='gemini-2.5-flash'):
        self.model_name = model_name
        
        system_instruction = """Sen bir uzman araÅŸtÄ±rmacÄ±sÄ±n.

GÃ–REV:
Verilen konuyu araÅŸtÄ±r, gÃ¼venilir kaynaklar bul, iÃ§erikleri analiz et.

YETENEKLERÄ°N:
1. Web'de arama yap (search tool)
2. URL'lerden iÃ§erik Ã§ek (scraping)
3. Ä°Ã§erikleri Ã¶zetle ve sentezle
4. Ã‡eliÅŸkileri/farklÄ± gÃ¶rÃ¼ÅŸleri belirle

Ã‡IKTI FORMATI:
{
  "topic": "AraÅŸtÄ±rÄ±lan konu",
  "sources": [
    {"url": "...", "title": "...", "summary": "..."}
  ],
  "key_findings": ["Bulgu 1", "Bulgu 2", ...],
  "summary": "Genel Ã¶zet",
  "confidence": 1-5
}

JSON formatÄ±nda yanÄ±t ver.
"""
        
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            tools=[search_web_simple],  # Web search tool
            system_instruction=system_instruction
        )
    
    async def research_topic(
        self,
        topic: str,
        max_sources: int = 5,
        scrape_content: bool = True
    ) -> Dict:
        """
        Konuyu araÅŸtÄ±r
        
        Args:
            topic: AraÅŸtÄ±rÄ±lacak konu
            max_sources: Maksimum kaynak sayÄ±sÄ±
            scrape_content: URL iÃ§eriklerini scrape et mi?
        
        Returns:
            dict: AraÅŸtÄ±rma sonuÃ§larÄ± (source scores dahil)
        """
        print(f"\nğŸ” AraÅŸtÄ±rma baÅŸlÄ±yor: {topic}")
        log_agent_action("ResearcherAgent", "research_start", {"topic": topic[:50]})
        
        # Source scorer baÅŸlat
        scorer = SourceScorer()
        
        # 1. Web search yap (LLM tool kullanÄ±r)
        chat = self.model.start_chat(enable_automatic_function_calling=True)
        
        search_prompt = f"""Bu konuyu web'de ara ve ilgili kaynaklarÄ± bul: {topic}

En fazla {max_sources} kaynak kullan."""
        
        print("   ğŸ“¡ Web search...")
        response = chat.send_message(search_prompt)
        
        # LLM'in bulduÄŸu kaynaklarÄ± iÃ§eren yanÄ±tÄ± al
        initial_findings = response.text
        
        # Search'ten gelen kaynaklarÄ± al ve skorla
        search_results = search_web_simple(topic, max_sources)
        scored_sources = scorer.score_multiple_sources(search_results)
        
        print(f"   âœ… Ä°lk bulgular toplandÄ± ({len(scored_sources)} kaynak)")
        
        # Rate limit: 5 req/min = 12 saniye arasÄ± gerekli
        time.sleep(13)
        
        # 2. EÄŸer scraping istenmiÅŸse, URL'leri scrape et
        scraped_data = []
        if scrape_content:
            urls = [s['url'] for s in scored_sources[:3]]  # En gÃ¼venilir 3 URL
            
            print(f"   ğŸ•·ï¸  {len(urls)} URL scraping...")
            scraped_data = await self._scrape_urls(urls)
            print(f"   âœ… {len(scraped_data)} URL scrape edildi")
        
        # 3. Final analiz: TÃ¼m verileri sentezle
        print("   ğŸ§  Final analiz...")
        analysis = self._synthesize_findings(
            topic=topic,
            initial_findings=initial_findings,
            scraped_data=scraped_data
        )
        
        # Kaynak skorlarÄ±nÄ± ekle
        analysis['scored_sources'] = scored_sources
        analysis['source_diversity'] = scorer.calculate_diversity_score(scored_sources)
        
        log_agent_action("ResearcherAgent", "research_complete", {
            "sources_found": len(scored_sources),
            "avg_score": sum(s['score'] for s in scored_sources) / len(scored_sources) if scored_sources else 0
        })
        
        print(f"   âœ… AraÅŸtÄ±rma tamamlandÄ±!\n")
        
        return analysis
    
    async def _scrape_urls(self, urls: List[str]) -> List[Dict]:
        """URL'leri scrape et"""
        results = []
        
        async with AsyncWebCrawler(verbose=False) as crawler:
            for url in urls:
                try:
                    result = await crawler.arun(url=url)
                    
                    if result.success:
                        # Ä°lk 3000 karakter al
                        content = result.markdown[:3000]
                        results.append({
                            'url': url,
                            'content': content,
                            'success': True
                        })
                    else:
                        results.append({
                            'url': url,
                            'error': result.error_message,
                            'success': False
                        })
                except Exception as e:
                    results.append({
                        'url': url,
                        'error': str(e),
                        'success': False
                    })
                
                # Rate limiting
                await asyncio.sleep(0.5)
        
        return results
    
    def _synthesize_findings(
        self,
        topic: str,
        initial_findings: str,
        scraped_data: List[Dict]
    ) -> Dict:
        """TÃ¼m bulgularÄ± sentezle"""
        
        # Synthesis prompt
        synthesis_prompt = f"""Konu: {topic}

Web Search BulgularÄ±:
{initial_findings}

"""
        
        # Scrape edilen iÃ§erikleri ekle
        if scraped_data:
            synthesis_prompt += "\nScrape Edilen Ä°Ã§erikler:\n"
            for i, data in enumerate(scraped_data, 1):
                if data['success']:
                    synthesis_prompt += f"\n[Kaynak {i}] {data['url']}\n{data['content'][:500]}...\n"
        
        synthesis_prompt += """

TÃ¼m bu bilgileri analiz et ve ÅŸu formatta JSON dÃ¶ndÃ¼r:
{
  "topic": "...",
  "key_findings": ["Ana bulgu 1", "Ana bulgu 2", ...],
  "summary": "Genel Ã¶zet (3-4 paragraf)",
  "sources_analyzed": sayÄ±,
  "confidence": 1-5 (veri kalitesine gÃ¶re)
}
"""
        
        # Synthesis model (JSON zorla)
        synthesis_model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config={
                "temperature": 0.5,
                "response_mime_type": "application/json"
            }
        )
        
        response = synthesis_model.generate_content(synthesis_prompt)
        
        # Rate limit iÃ§in bekle
        time.sleep(13)
        
        import json
        analysis = json.loads(response.text)
        
        # Scrape edilen URL'leri ekle
        analysis['scraped_sources'] = [
            {'url': d['url'], 'success': d['success']}
            for d in scraped_data
        ]
        
        return analysis


# =============================================================================
# TEST
# =============================================================================

async def test_researcher():
    print("\n" + "="*70)
    print("ğŸ§ª RESEARCHER AGENT TEST")
    print("="*70 + "\n")
    
    # Agent oluÅŸtur
    researcher = ResearcherAgent()
    
    # Test
    topic = "Yapay zeka gÃ¼venliÄŸi ve etik"
    
    print(f"ğŸ“‹ Konu: {topic}\n")
    
    # AraÅŸtÄ±r (scraping kapalÄ± - Ã§ok uzun sÃ¼rer)
    result = await researcher.research_topic(
        topic=topic,
        max_sources=3,
        scrape_content=False  # Test iÃ§in kapalÄ±
    )
    
    # SonuÃ§larÄ± gÃ¶ster
    print("\n" + "="*70)
    print("ğŸ“Š ARAÅTIRMA SONUÃ‡LARI")
    print("="*70 + "\n")
    
    print(f"ğŸ“Œ Konu: {result['topic']}")
    print(f"ğŸ”¢ Kaynak sayÄ±sÄ±: {result['sources_analyzed']}")
    print(f"â­ GÃ¼ven: {result['confidence']}/5\n")
    
    print("ğŸ”‘ Ana Bulgular:")
    for i, finding in enumerate(result['key_findings'], 1):
        print(f"{i}. {finding}")
    
    print(f"\nğŸ“ Ã–zet:\n{result['summary']}\n")
    
    # Kaydet
    import json
    output_file = "examples/output_researcher_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ Kaydedildi: {output_file}\n")
    
    print("="*70)
    print("âœ… TEST TAMAMLANDI")
    print("="*70 + "\n")


if __name__ == "__main__":
    asyncio.run(test_researcher())
